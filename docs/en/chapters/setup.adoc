=== Operating System ===

Install the operating system (Debian Stretch) on the master:

----
./sdcard/jarvis_master_os.sh -s /dev/mmcblk0
== Jarvis OS: Rock64 Stretch 0.5.15 ==
Download OS image
Install OS on /dev/mmcblk0
2218786816 bytes (2.2 GB, 2.1 GiB) copied, 10 s, 222 MB/s
544+1 records in
544+1 records out
2282749952 bytes (2.3 GB, 2.1 GiB) copied, 42.0649 s, 54.3 MB/s
== Done ==
----

Install https://github.com/hypriot/image-builder-rpi/releases[HypriotOS] onto
the sdcard:

----
$ sdcard/jarvis_os_2.sh jarvis myssid mywifipassword Linux
----

Generate a new machineid on each host, see : https://github.com/hypriot/image-builder-rpi/issues/167


=== Cluster ===

https://www.ansible.com/[Ansible] is used to configure the *Salt* infrastructure.

Go into the *ansible* directory

==== Ansible ====

Create an *inventory* file.

Example, my personal configuration is :

* a kubernetes cluster (Rock64 as master, 3 RPI3 as nodes)
* PI ZeroW as Home Assistant server
* PI3 as a multimedia server (OSMC)

----
[master]
<master_ip_address>         ansible_connection=ssh        ansible_user=pirate

[nodes]
<node1_ip_address>          ansible_connection=ssh        ansible_user=pirate
<node2_ip_address>          ansible_connection=ssh        ansible_user=pirate
<node3_ip_address>          ansible_connection=ssh        ansible_user=pirate

[osmc]
<osmc_ip_address>          ansible_connection=ssh        ansible_user=pirate
----

You could now check communications with hosts:

----
$ make check
----


==== Saltstack ====

Saltstack is used to manage all hosts.
Install the master :

----
$ make salt-master
----

and the minions :

----
$ make salt-minions
----

On the master accepts minions keys. See https://docs.saltstack.com/en/latest/ref/cli/salt-key.html

On the master :

----
$ cd /srv/jarvis
$ make check
$ make hosts
----

Install Kubernetes dependencies :

----
$ make k8s-setup
----

You could check hosts setup :

----
$ make k8s-check
[Jarvis / Salt] Check hosts
jarvis-master:
    ii  docker-ce                                                       18.06.1~ce~3-0~ubuntu                    arm64        Docker: the open-source application container engine
jarvis-node3.localdomain:
    ii  docker-ce                     18.06.1~ce~3-0~debian          arm64        Docker: the open-source application container engine
jarvis-node2.localdomain:
    ii  docker-ce                     18.06.1~ce~3-0~debian          arm64        Docker: the open-source application container engine
jarvis-node1.localdomain:
    ii  docker-ce                     18.06.1~ce~3-0~debian          arm64        Docker: the open-source application container engine
jarvis-master:
    ii  kubeadm                                                         1.13.3-00                                arm64        Kubernetes Cluster Bootstrapping Tool
    ii  kubectl                                                         1.13.3-00                                arm64        Kubernetes Command Line Tool
    ii  kubelet                                                         1.13.3-00                                arm64        Kubernetes Node Agent
    ii  kubernetes-cni                                                  0.6.0-00                                 arm64        Kubernetes CNI
jarvis-node2.localdomain:
    ii  kubeadm                       1.13.3-00                      arm64        Kubernetes Cluster Bootstrapping Tool
    ii  kubectl                       1.13.3-00                      arm64        Kubernetes Command Line Tool
    ii  kubelet                       1.13.3-00                      arm64        Kubernetes Node Agent
    ii  kubernetes-cni                0.6.0-00                       arm64        Kubernetes CNI
jarvis-node3.localdomain:
    ii  kubeadm                       1.13.3-00                      arm64        Kubernetes Cluster Bootstrapping Tool
    ii  kubectl                       1.13.3-00                      arm64        Kubernetes Command Line Tool
    ii  kubelet                       1.13.3-00                      arm64        Kubernetes Node Agent
    ii  kubernetes-cni                0.6.0-00                       arm64        Kubernetes CNI
jarvis-node1.localdomain:
    ii  kubeadm                       1.13.3-00                      arm64        Kubernetes Cluster Bootstrapping Tool
    ii  kubectl                       1.13.3-00                      arm64        Kubernetes Command Line Tool
    ii  kubelet                       1.13.3-00                      arm64        Kubernetes Node Agent
    ii  kubernetes-cni                0.6.0-00                       arm64        Kubernetes CNI
----

=== Components ===

==== Cluster ====

On the master initialize the cluster :

----
$ sudo kubeadm config images pull --kubernetes-version v1.13.3
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.13.3
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.13.3
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.13.3
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.13.3
[config/images] Pulled k8s.gcr.io/pause:3.1
[config/images] Pulled k8s.gcr.io/etcd:3.2.24
[config/images] Pulled k8s.gcr.io/coredns:1.2.6

$ sudo kubeadm init --kubernetes-version v1.13.8 \
    --apiserver-advertise-address=192.168.1.33 \
    --pod-network-cidr=10.244.0.0/16 \
    --skip-preflight-checks

[...]

kubeadm join 192.168.1.33:6443 --token zgr9qr.ek2obrpm191294nu --discovery-token-ca-cert-hash sha256:335b8f0b47864b3bd2b55fdae64eb97454eb3baeab8b3dc1b3ad29a7973b1336
----

Wait for pods are available. Then install the CNI.
We use Flannel due to an error on Weave (RPI reboot) :

----
$ curl -sSL https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml | sed 's/amd64/arm64/g' | kubectl create -f -
----


Check logs. If there is an error with CNI plugin, check if file **/etc/cni/net.d/10-flannel.conf** exists.
Create it with :

----
{
  "name": "cbr0",
  "type": "flannel",
  "delegate": {
    "isDefaultGateway": true
  }
}
----


Check the join command from the master :

----
$ sudo kubeadm token create --print-join-command
kubeadm join 192.168.1.33:6443 --token zgr9qr.ek2obrpm191294nu --discovery-token-ca-cert-hash sha256:335b8f0b47864b3bd2b55fdae64eb97454eb3baeab8b3dc1b3ad29a7973b1336
----

Then on each node, join the cluster :

----
$ sudo kubeadm join 192.168.1.33:6443 --token zgr9qr.ek2obrpm191294nu --discovery-token-ca-cert-hash sha256:335b8f0b47864b3bd2b55fdae64eb97454eb3baeab8b3dc1b3ad29a7973b1336
----

Wait for control plane is available :

----
$ kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}
----

How cluster's nodes are :

----
$ kubectl get nodes -o wide
NAME            STATUS    ROLES     AGE       VERSION   EXTERNAL-IP   OS-IMAGE                       KERNEL-VERSION               CONTAINER-RUNTIME
jarvis-master   Ready     master    2d        v1.9.6    <none>        Ubuntu 16.04.3 LTS             4.4.77-rockchip-ayufan-136   docker://18.3.1
jarvis-node1    Ready     <none>    2d        v1.9.6    <none>        Debian GNU/Linux 9 (stretch)   4.14.37-hypriotos-v8         docker://18.4.0
jarvis-node2    Ready     <none>    2d        v1.9.6    <none>        Debian GNU/Linux 9 (stretch)   4.14.37-hypriotos-v8         docker://18.4.0
jarvis-node3    Ready     <none>    2d        v1.9.6    <none>        Debian GNU/Linux 9 (stretch)   4.14.37-hypriotos-v8         docker://18.4.0
----

==== Kubernetes Dashboard ====

You could install the official Kubernetes Dashboard :

----
$ kubectl apply -f k8s/dashboard --record
$ kubectl describe services kubernetes-dashboard --namespace=kube-system
----

==== DNS ====

You could replace the kube-dns default installation with https://coredns.io/[CoreDNS] :

----
$ kubectl apply -f k8s/coredns --record
$ kubectl describe services kube-dns --namespace=kube-system
Name:             kube-dns
Namespace:        kube-system
Labels:           k8s-app=coredns
                       kubernetes.io/cluster-service=true
                       kubernetes.io/name=CoreDNS
Annotations:      kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"coredns","kubernetes.io/cluster-service":"true","kubernetes.io/na...
Selector:          k8s-app=coredns
Type:              ClusterIP
IP:                10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.36.0.5:53,10.44.0.2:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.36.0.5:53,10.44.0.2:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.36.0.5:9153,10.44.0.2:9153
Session Affinity:  None
Events:            <none>
----

==== Heapster ====

Heapster enables Container Cluster Monitoring and Performance Analysis for Kubernetes :

----
$ kubectl apply -f k8s/heapster --record
----

==== Ingress Controllers ====

Nginx is used as the default Ingress Controller :

----
$ kubectl apply  -f ingress/default-backend.yaml --record
$ kubectl apply  -f ingress/nginx/ --record
----


==== MetalLB ====

https://metallb.universe.tf/[MetalLB] is used as a load-balancer for services. Weâ€™ll assume the cluster is set up on a network using **192.168.2.224/27**

----
$ kubectl apply -f k8s/metallb/metallb.yaml
namespace "metallb-system" created
clusterrole "metallb-system:controller" created
clusterrole "metallb-system:speaker" created
role "leader-election" created
role "config-watcher" created
serviceaccount "controller" created
serviceaccount "speaker" created
clusterrolebinding "metallb-system:controller" created
clusterrolebinding "metallb-system:speaker" created
rolebinding "config-watcher" created
rolebinding "leader-election" created
deployment "controller" created
daemonset "speaker" created

$ kubectl apply -f k8s/metallb/configmap.yaml
configmap "config" created
----

You could check that an IP is setup for the Nginx service :

----
$ kubectl get svc --all-namespaces -l app=nginx-ingress-lb
NAMESPACE        NAME               TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE
ingress-system   nginx-ingress-lb   LoadBalancer   10.102.221.227   192.168.1.224   80:32510/TCP   1h
----

And check response :

----
$ curl -vs -i 192.168.1.224:80/healthz | head -n 1
* Hostname was NOT found in DNS cache
*   Trying 192.168.1.224...
* Connected to 192.168.1.224 (192.168.1.224) port 80 (#0)
> GET /healthz HTTP/1.1
> User-Agent: curl/7.38.0
> Host: 192.168.1.224
> Accept: */*
>
< HTTP/1.1 200 OK
* Server nginx/1.13.9 is not blacklisted
< Server: nginx/1.13.9
< Date: Mon, 05 Mar 2018 15:59:22 GMT
< Content-Type: text/html
< Content-Length: 0
< Connection: keep-alive
< Strict-Transport-Security: max-age=15724800; includeSubDomains;
<
* Connection #0 to host 192.168.1.224 left intact
HTTP/1.1 200 OK
----


==== Status ====

After a few minutes, check the cluster informations :

----
$ kubectl cluster-info
Kubernetes master is running at https://192.168.1.36:6443
Heapster is running at https://192.168.1.36:6443/api/v1/namespaces/kube-system/services/heapster/proxy
CoreDNS is running at https://192.168.1.36:6443/api/v1/namespaces/kube-system/services/kube-dns/proxy
----

You can get a simple diagnostic:

----
$ kubectl get componentstatus
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}
----

You could see also nodes metrics (with heapster) :

----
$ kubectl top nodes
NAME            CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
jarvis-master   631m         15%       639Mi           83%
jarvis-node2    216m         5%        485Mi           63%
jarvis-node1    254m         6%        531Mi           69%
----


=== Administration ===

==== Security ====

**TODO**

==== Quotas ====

**TODO**

==== Backup ====

**TODO**

==== Validation ====

**TODO**
